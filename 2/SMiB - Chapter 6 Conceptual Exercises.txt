SMiB - Chapter 6 Conceptual Exercises

***************************************************************************
6.8.1: 
We perform best subset, forward stepwise, and backward stepwise
selection on a single data set. For each approach, we obtain p + 1
models, containing 0, 1, 2, . . . , p predictors. Explain your answers:

(a) Which of the three models with k predictors has the smallest
training RSS?

Best subset selection: the model with k predictors is the model with the smallest training RSS

(b) Which of the three models with k predictors has the smallest
test RSS?

It's not possible to tell - the fwd/bwd selection might select a better model by sheer luck, but the best subset selection will evaluate more models

(c) True or False:
i. The predictors in the k-variable model identified by forward
stepwise are a subset of the predictors in the (k+1)-variable
model identified by forward stepwise selection.

True; the predictors in the (k+1)-variable model in fwd selection are the same predictors in the k-variable model, plus one additional step/predictor.

ii. The predictors in the k-variable model identified by backward
stepwise are a subset of the predictors in the (k + 1)-
variable model identified by backward stepwise selection.

True; the predictors in the k-variable model in bwd selection are the same predictors in the (k+1)-variable model, minus one additional step/predictor.

iii. The predictors in the k-variable model identified by backward
stepwise are a subset of the predictors in the (k + 1)-
variable model identified by forward stepwise selection.

False; predictors chosen at k and (k+1) steps for each of bwd/fwd selection may be different

iv. The predictors in the k-variable model identified by forward
stepwise are a subset of the predictors in the (k+1)-variable
model identified by backward stepwise selection.

False; predictors chosen at k and (k+1) steps for each of bwd/fwd selection may be different

v. The predictors in the k-variable model identified by best
subset are a subset of the predictors in the (k + 1)-variable
model identified by best subset selection.

False; depending on if the best subset chooses to add or subtract a predictor from the model at (k+1), this may be the case

***************************************************************************
6.8.2:
For parts (a) through (c), indicate which of i. through iv. is correct.
Justify your answer.
(a) The lasso, relative to least squares, is:
	Less flexible and hence will give improved prediction accuracy
	when its increase in bias is less than its decrease in
	variance.
(b) Repeat (a) for ridge regression relative to least squares.
	Less flexible and hence will give improved prediction accuracy
	when its increase in bias is less than its decrease in
	variance.
(c) Repeat (a) for non-linear methods relative to least squares.
	More flexible and hence will give improved prediction accuracy
	when its increase in variance is less than its decrease
	in bias.

***************************************************************************
6.8.5:
It is well-known that ridge regression tends to give similar coefficient
values to correlated variables, whereas the lasso may give quite different
coefficient values to correlated variables. We will now explore
this property in a very simple setting.

Suppose that n = 2, p = 2, x11 = x12, x21 = x22. Furthermore,
suppose that y1+y2 = 0 and x11+x21 = 0 and x12+x22 = 0, so that
the estimate for the intercept in a least squares, ridge regression, or
lasso model is zero: ˆ β0 = 0.
(a) Write out the ridge regression optimization problem in this setting.



(b) Argue that in this setting, the ridge coefficient estimates satisfy
ˆ β1 = ˆ β2.



(c) Write out the lasso optimization problem in this setting.



(d) Argue that in this setting, the lasso coefficients ˆ β1 and ˆ β2 are
not unique—in other words, there are many possible solutions
to the optimization problem in (c). Describe these solutions.




***************************************************************************
6.8.8:

***************************************************************************
6.8.10:








